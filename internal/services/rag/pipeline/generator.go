package pipeline

import (
	"context"
	"encoding/json"
	"fmt"

	"ocrserver/internal/config"
	"ocrserver/internal/consts"
	"ocrserver/internal/opensearch"

	"ocrserver/internal/services"
	"ocrserver/internal/services/ialib"
	"ocrserver/internal/utils/erros"
	"ocrserver/internal/utils/logger"

	"github.com/openai/openai-go/v2/responses"
)

type GeneratorType struct {
}

func NewGeneratorType() *GeneratorType {
	return &GeneratorType{}
}

func (service *GeneratorType) ExecutaAnaliseProcesso(
	ctx context.Context,
	idCtxt int,
	msgs ialib.MsgGpt,
	prevID string,
	autos []consts.ResponseAutosRow,
	ragBase []opensearch.ResponseBase) (string, []responses.ResponseOutputItemUnion, error) {

	messages := ialib.MsgGpt{}

	// Valida√ß√£o inicial
	if len(autos) == 0 {
		logger.Log.Warningf("Autos do processo est√£o vazios (id_ctxt=%d)", idCtxt)
		return "", nil, erros.CreateError("Os autos do processo est√£o vazios")
	}
	//01 - DEVELOPER
	messages.AddMessage(ialib.MessageResponseItem{
		Id:   "",
		Role: "developer",
		Text: "Voc√™ √© um assistente jur√≠dico especializado em an√°lise de processos judiciais. Siga estritamente o formato JSON definido.",
	})

	// 02 - RAG: Base de conhecimento (RAG)

	if len(ragBase) > 0 {
		logger.Log.Info("Acrescentando a base de conhecimento")
		// txtRag := `A seguir, apresento informa√ß√µes jur√≠dicas relevantes e casos semelhantes, extra√≠dos de nossa
		// base de conhecimento. Use essas informa√ß√µes apenas como refer√™ncia para fundamentar a an√°lise do processo,
		// sem criar novos fatos.`
		const RAGHeader = `As informa√ß√µes a seguir foram recuperadas de nossa base de conhecimento jur√≠dica (RAG).
			Elas cont√™m fundamentos e temas relevantes de casos semelhantes.
			Utilize-as apenas como refer√™ncia para an√°lise jur√≠dica, sem criar novos fatos.`
		messages.AddMessage(ialib.MessageResponseItem{
			Id:   "",
			Role: "user",
			Text: RAGHeader,
		})

		for _, doc := range ragBase {
			//texto := doc.DataTexto
			texto := doc.Tema + ": " + doc.DataTexto
			tokens, _ := ialib.OpenaiGlobal.StringTokensCounter(texto)
			if tokens > MAX_DOC_TOKENS { // preven√ß√£o contra prompts gigantes
				texto = texto[:MAX_DOC_TOKENS] + "...(truncado)"
				logger.Log.Infof("doutrina com %d tokens > %d: %s", tokens, MAX_DOC_TOKENS, doc.Tema)
			}
			messages.AddMessage(ialib.MessageResponseItem{
				Id:   doc.Id,
				Role: "user",
				Text: texto,
			})
			// logger.Log.Infof("\nTema: %s", doc.Tema)
			// logger.Log.Infof("\nTexto: %s", doc.DataTexto)
		}

	} else {
		logger.Log.Info("Doutrina est√° vazia")
	}

	// 03 - PROMPT: Obt√©m o prompt que ir√° orientar a an√°lise e elabora√ß√£o da senten√ßa
	prompt, err := services.PromptServiceGlobal.GetPromptByNatureza(consts.PROMPT_RAG_ANALISE)
	if err != nil {
		logger.Log.Errorf("Erro ao buscar prompt (id_ctxt=%d): %v", idCtxt, err)
		return "", nil, erros.CreateError("Erro ao buscar prompt: %s", err.Error())
	}
	//logger.Log.Infof("prompt: %s", prompt)

	// Adiciona como a primeira mensagem
	messages.AddMessage(ialib.MessageResponseItem{
		Id:   "",
		Role: "user",
		Text: prompt,
	})

	// 04 - AUTOS: Autos processuais
	for _, doc := range autos {
		texto := doc.DocJsonRaw
		tokens, _ := ialib.OpenaiGlobal.StringTokensCounter(texto)
		if tokens > MAX_DOC_TOKENS { // preven√ß√£o contra prompts gigantes
			texto = texto[:MAX_DOC_TOKENS] + "...(truncado)"
			logger.Log.Infof("pe√ßa processual com %d tokens  > %d: %s", tokens, MAX_DOC_TOKENS, doc.IdPje)
		}
		messages.AddMessage(ialib.MessageResponseItem{
			Id:   "",
			Role: "user",
			Text: texto,
		})
	}

	// PROMPT DO USU√ÅRIO
	for _, msg := range msgs.Messages {
		messages.AddMessage(ialib.MessageResponseItem{
			Id:   msg.Id,
			Role: msg.Role,
			Text: msg.Text,
		})
	}

	// Chamada ao servi√ßo OpenAI
	resp, err := services.OpenaiServiceGlobal.SubmitPromptResponse(
		ctx,
		messages, prevID,
		config.GlobalConfig.OpenOptionModel,
		ialib.REASONING_LOW,
		ialib.VERBOSITY_LOW,
	)

	if err != nil {
		logger.Log.Errorf("Erro ao submeter an√°lise (id_ctxt=%d): %v", idCtxt, err)
		return "", nil, erros.CreateError("Erro ao submeter an√°lise: %s", err.Error())
	}

	if resp == nil {
		logger.Log.Errorf("Resposta nula recebida do servi√ßo OpenAI (id_ctxt=%d)", idCtxt)
		return "", nil, erros.CreateError("Resposta nula recebida do servi√ßo OpenAI")
	}

	// Atualiza uso de tokens

	services.ContextoServiceGlobal.UpdateTokenUso(
		idCtxt,
		int(resp.Usage.InputTokens),
		int(resp.Usage.OutputTokens),
	)

	return resp.ID, resp.Output, nil
}

func (service *GeneratorType) ExecutaAnaliseJulgamento(ctx context.Context,
	idCtxt int,
	msgs ialib.MsgGpt,
	prevID string,
	autos []consts.ResponseAutosRow,
	ragBase []opensearch.ResponseBase) (string, []responses.ResponseOutputItemUnion, error) {

	// Constru√ß√£o das mensagens
	messages := ialib.MsgGpt{}

	//01 - Contexto base de conhecimento (RAG)
	messages.AddMessage(ialib.MessageResponseItem{
		Id:   "",
		Role: "developer",
		Text: `Voc√™ √© um assistente jur√≠dico especializado em an√°lise de processos judiciais. 
		Siga estritamente o formato JSON e as regras fornecidas.`,
	})

	//02 - RAG: Acrescento a base de conhecimento RAG
	const RAGHeader = `As informa√ß√µes a seguir foram recuperadas de nossa base de conhecimento jur√≠dica (RAG).
	Elas cont√™m fundamentos e temas relevantes de casos semelhantes.
	Utilize-as apenas como refer√™ncia para an√°lise jur√≠dica, sem criar novos fatos.`

	// txtRag := `A seguir, apresento informa√ß√µes jur√≠dicas relevantes e casos semelhantes, extra√≠dos de nossa
	// 	base de conhecimento. Use essas informa√ß√µes apenas como refer√™ncia para fundamentar a an√°lise do processo,
	// 	sem criar novos fatos.`
	messages.AddMessage(ialib.MessageResponseItem{
		Id:   "",
		Role: "user",
		Text: RAGHeader,
	})
	for _, doc := range ragBase {
		texto := doc.DataTexto
		tokens, _ := ialib.OpenaiGlobal.StringTokensCounter(texto)
		if tokens > MAX_DOC_TOKENS { // preven√ß√£o contra documentos gigantes
			texto = texto[:MAX_DOC_TOKENS] + "...(truncado)"
			logger.Log.Infof("doutrina com %d tokens > %d: %s", tokens, MAX_DOC_TOKENS, doc.Tema)
		}
		messages.AddMessage(ialib.MessageResponseItem{
			Id:   "",
			Role: "user",
			Text: texto,
		})
		logger.Log.Infof("\nTema: %s", doc.Tema)
		logger.Log.Infof("\nTexto: %s", doc.DataTexto)
	}
	if len(ragBase) == 0 {
		logger.Log.Info("N√£o foram obtidos registros da base de conhecimento: ragBase==0")
	}

	//03 - PROMPTO: Obt√©m o prompt que ir√° orientar a pr√©-an√°lise e elabora√ß√£o da senten√ßa
	prompt, err := services.PromptServiceGlobal.GetPromptByNatureza(consts.PROMPT_RAG_JULGAMENTO)
	if err != nil {
		logger.Log.Errorf("Erro ao buscar o prompt: %v", err)
		return "", nil, erros.CreateError("Erro ao buscar PROMPT_RAG_COMPLEMENTO", err.Error())
	}
	//logger.Log.Infof("prompt: %s", prompt)
	messages.AddMessage(ialib.MessageResponseItem{
		Id:   "",
		Role: "user",
		Text: prompt,
	})

	for _, doc := range autos {
		texto := doc.DocJsonRaw
		tokens, _ := ialib.OpenaiGlobal.StringTokensCounter(texto)
		if tokens > MAX_DOC_TOKENS { // preven√ß√£o contra documentos gigantes
			texto = texto[:MAX_DOC_TOKENS] + "...(truncado)"
			logger.Log.Infof("doutrina com %d tokens > %d: %s", tokens, MAX_DOC_TOKENS, doc.IdPje)
		}
		messages.AddMessage(ialib.MessageResponseItem{
			Id:   "",
			Role: "user",
			Text: texto,
		})

	}

	// Mensagens do usu√°rio
	for _, msg := range msgs.Messages {
		messages.AddMessage(ialib.MessageResponseItem{
			Id:   msg.Id,
			Role: msg.Role,
			Text: msg.Text,
		})
	}

	// Chamada ao servi√ßo OpenAI
	resp, err := services.OpenaiServiceGlobal.SubmitPromptResponse(
		ctx,
		messages, prevID,
		config.GlobalConfig.OpenOptionModel,
		ialib.REASONING_LOW,
		ialib.VERBOSITY_LOW)
	if err != nil {
		logger.Log.Errorf("Erro ao submeter an√°lise (id_ctxt=%d): %v", idCtxt, err)
		return "", nil, erros.CreateError("Erro ao submeter an√°lise: %s", err.Error())
	}

	if resp == nil {
		logger.Log.Errorf("Resposta nula recebida do servi√ßo OpenAI (id_ctxt=%d)", idCtxt)
		return "", nil, erros.CreateError("Resposta nula recebida do servi√ßo OpenAI")
	}
	// Atualiza uso de tokens

	services.ContextoServiceGlobal.UpdateTokenUso(
		idCtxt,
		int(resp.Usage.InputTokens),
		int(resp.Usage.OutputTokens),
	)

	return resp.ID, resp.Output, nil
}

func (service *GeneratorType) VerificaQuestoesControvertidas(
	ctx context.Context,
	id_ctxt int,
	msgs ialib.MsgGpt,
	prevID string,
) (string, []responses.ResponseOutputItemUnion, error) {

	retriObj := NewRetrieverType()

	// üîπ Recupera pr√©-an√°lise
	preAnalise, err := retriObj.RecuperaPreAnaliseJudicial(ctx, id_ctxt)
	if err != nil {
		logger.Log.Errorf("[id_ctxt=%d] Erro ao realizar busca de pr√©-an√°lise: %v", id_ctxt, err)
		return "", nil, erros.CreateError("Erro ao buscar pr√©-an√°lise: %s", err.Error())
	}
	if len(preAnalise) == 0 {
		logger.Log.Warningf("[id_ctxt=%d] Nenhuma pr√©-an√°lise encontrada", id_ctxt)
		return "", nil, erros.CreateError("N√£o foi realizada a pr√©-an√°lise.")
	}

	// üîπ Obt√©m o prompt de verifica√ß√£o
	prompt, err := services.PromptServiceGlobal.GetPromptByNatureza(consts.PROMPT_RAG_COMPLEMENTA_JULGAMENTO)
	if err != nil {
		logger.Log.Errorf("[id_ctxt=%d] Erro ao buscar prompt: %v", id_ctxt, err)
		return "", nil, erros.CreateError("Erro ao buscar prompt: %s", err.Error())
	}

	// üß± Cria novo objeto de mensagens preservando hist√≥rico
	var msgsAtual ialib.MsgGpt
	for _, m := range msgs.Messages {
		msgsAtual.AddMessage(m) // adiciona hist√≥rico anterior
	}

	// üîπ Adiciona o prompt (como system ou developer) mantendo hist√≥rico anterior
	msgsAtual.AddMessage(ialib.MessageResponseItem{
		Role: "developer",
		Text: prompt,
	})

	// üîπ Converte pr√©-an√°lise para struct Go
	jsonObj := preAnalise[0].DocJsonRaw
	var objAnalise AnaliseJuridicaIA
	if err := json.Unmarshal([]byte(jsonObj), &objAnalise); err != nil {
		logger.Log.Errorf("[id_ctxt=%d] Erro ao realizar unmarshal da pr√©-an√°lise: %v", id_ctxt, err)
		return "", nil, erros.CreateError("Erro ao decodificar pr√©-an√°lise.")
	}

	// üîπ Adiciona quest√µes controvertidas como mensagens de usu√°rio
	for _, q := range objAnalise.QuestoesControvertidas {
		texto := fmt.Sprintf("Pergunta: %s", q.PerguntaAoUsuario)
		tokens, _ := ialib.OpenaiGlobal.StringTokensCounter(texto)
		if tokens > MAX_DOC_TOKENS {
			texto = texto[:MAX_DOC_TOKENS] + "...(truncado)"
			logger.Log.Infof("[id_ctxt=%d] Quest√£o truncada (%d tokens > %d)", id_ctxt, tokens, MAX_DOC_TOKENS)
		}
		msgsAtual.AddMessage(ialib.MessageResponseItem{
			Role: "user",
			Text: texto,
		})
	}

	// üîπ Submete o hist√≥rico completo (sem sobrescrever msgs)
	resp, err := services.OpenaiServiceGlobal.SubmitPromptResponse(
		ctx,
		msgsAtual, // ‚Üê mant√©m todas as mensagens acumuladas
		prevID,
		config.GlobalConfig.OpenOptionModel,
		ialib.REASONING_LOW,
		ialib.VERBOSITY_LOW,
	)
	if err != nil {
		logger.Log.Errorf("[id_ctxt=%d] Erro ao submeter prompt de verifica√ß√£o: %v", id_ctxt, err)
		return "", nil, erros.CreateError("Erro ao submeter prompt: %s", err.Error())
	}

	// üîπ Atualiza uso de tokens
	if resp != nil {
		usage := resp.Usage
		services.ContextoServiceGlobal.UpdateTokenUso(
			id_ctxt,
			int(usage.InputTokens),
			int(usage.OutputTokens),
		)
	}

	// üîπ Retorna resultado do modelo
	if resp == nil {
		return "", nil, erros.CreateError("Resposta nula recebida do modelo")
	}

	return resp.ID, resp.Output, err
}
